{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from functions.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zfkha\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\zfkha\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from functions import get_final_answer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zfkha\\miniconda3\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whining\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Fox\"\n",
    "#wiki_extract = get_wiki_extract(url)\n",
    "#print(wiki_extract)\n",
    "question = \"What does the fox say?\"\n",
    "answer = get_final_answer(question=question,url=url)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"squad_v2\",split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"id\", axis=1)\n",
    "df = df.drop(\"context\",axis=1)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['answers'].apply(lambda x: x['text'])\n",
    "df = df.drop(df[df['text'].apply(lambda x: len(x) == 0)].index)\n",
    "df = df.drop('answers', axis=1)\n",
    "df.rename(columns = {'text':'answers'}, inplace = True)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sample(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for index, row in df2.iterrows():\n",
    "    url = \"https://en.wikipedia.org/wiki/\" + row[\"title\"]\n",
    "    #print(url)\n",
    "    question = row[\"question\"]\n",
    "    #print(question)\n",
    "    answer = get_final_answer(question=question,url=url)\n",
    "    answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_answers = pd.DataFrame(df2[\"answers\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true_answers, predicted_answers):\n",
    "    \"\"\"\n",
    "    Calculates the F1 score given the true answers and predicted answers.\n",
    "    Both true_answers and predicted_answers are lists of strings.\n",
    "    \"\"\"\n",
    "    # Convert predicted_answers list to a set of tuples\n",
    "    predicted_answers_set = set(predicted_answers)\n",
    "\n",
    "    # Calculate precision\n",
    "    if len(predicted_answers_set) == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        true_positives = 0\n",
    "        for ans_p in predicted_answers_set:\n",
    "            for ans_t in true_answers:\n",
    "                if all([a in ans_p for a in ans_t.split()]):\n",
    "                    true_positives += 1\n",
    "                    break  # go to next predicted answer if a true answer is found\n",
    "        precision = true_positives / len(predicted_answers_set)\n",
    "\n",
    "    # Calculate recall\n",
    "    true_positives = 0\n",
    "    for ans_t in true_answers:\n",
    "        for ans_p in predicted_answers_set:\n",
    "            if all([a in ans_t for a in ans_p]):\n",
    "                true_positives += 1\n",
    "                break  # go to next true answer if a predicted answer is found\n",
    "    recall = true_positives / len(true_answers)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    if precision == 0 and recall == 0:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "for i in range(0,len(answers)):\n",
    "    answer = answers[i]\n",
    "    correct_answer = correct_answers[i]\n",
    "    #print(answer)\n",
    "    #print(correct_answer)\n",
    "    for inner_list in correct_answer:\n",
    "        for element in inner_list:\n",
    "            if answer in element or element in answer:\n",
    "                pos+=1\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "        else:\n",
    "            neg+=1\n",
    "acc_rate = 100*(pos/(pos+neg))\n",
    "print(\"accuracy rate = \",acc_rate,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for i in range(0, len(answers)):\n",
    "    answer = answers[i]\n",
    "    correct_answer = correct_answers[i]\n",
    "    is_match = False\n",
    "    for inner_list in correct_answer:\n",
    "        for element in inner_list:\n",
    "            if answer in element or element in answer:\n",
    "                is_match = True\n",
    "                break\n",
    "        if is_match:\n",
    "            break\n",
    "    if is_match:\n",
    "        true_positives += 1\n",
    "    else:\n",
    "        false_negatives += 1\n",
    "\n",
    "    is_match = False\n",
    "    for inner_list in correct_answer:\n",
    "        for element in inner_list:\n",
    "            #print(element)\n",
    "            #print(str(correct_answer))\n",
    "            if element in correct_answer or str(correct_answer) in element:\n",
    "                is_match = True\n",
    "                print(element)\n",
    "                print(str(correct_answer))\n",
    "                break\n",
    "        if is_match:\n",
    "            break\n",
    "    if not is_match:\n",
    "        false_positives += 1\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(true_positives)\n",
    "print(false_positives)\n",
    "print(false_negatives)\n",
    "\n",
    "print(\"precision = \", precision)\n",
    "print(\"recall = \", recall)\n",
    "print(\"F1 score = \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triviaqa = pd.read_json(\"training.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triviaqa = triviaqa.drop(\"version\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triviaqa['title'] = triviaqa['data'].apply(lambda x: x['title'])\n",
    "triviaqa = triviaqa.drop(df[df['title'].apply(lambda x: len(x) == 0)].index)\n",
    "triviaqa['paragraphs'] = triviaqa['data'].apply(lambda x: x['paragraphs'][0])\n",
    "triviaqa['qas'] = triviaqa['paragraphs'].apply(lambda x: x['qas'][0])\n",
    "triviaqa['question'] = triviaqa['qas'].apply(lambda x: x['question'])\n",
    "triviaqa['answers'] = triviaqa['qas'].apply(lambda x: x['answers'])\n",
    "triviaqa = triviaqa.drop(triviaqa[triviaqa['answers'].apply(lambda x: len(x) == 0)].index)\n",
    "triviaqa['answers'] = triviaqa['qas'].apply(lambda x: x['answers'][0])\n",
    "triviaqa['answer'] = triviaqa['answers'].apply(lambda x: x['text'])\n",
    "triviaqa = triviaqa.drop(['data','paragraphs','qas','answers'], axis=1)\n",
    "#triviaqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = triviaqa.sample(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers2 = []\n",
    "questions2 = []\n",
    "correct_answers2 = []\n",
    "for index, row in df3.iterrows():\n",
    "    #print(row[\"title\"])\n",
    "    #print(row[\"question\"])\n",
    "    url = \"https://en.wikipedia.org/wiki/\" + row[\"title\"]\n",
    "    #print(url)\n",
    "    question = row[\"question\"]\n",
    "    #print(question)\n",
    "    answer = get_final_answer(question=question,url=url)\n",
    "    correct_answer = row['answer']\n",
    "    correct_answers2.append(correct_answer)\n",
    "    questions2.append(question)\n",
    "    answers2.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questions2)\n",
    "print(answers2)\n",
    "print(correct_answers2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/\" + \"Central_Intelligence_Agency\"\n",
    "#print(url)\n",
    "question = \"What is the CIA's main focus?\"\n",
    "#print(question)\n",
    "answer = get_final_answer(question=question,url=url)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_questions = []\n",
    "i = 0\n",
    "\n",
    "for index, row in df3.iterrows():\n",
    "    answer = row['answer']\n",
    "    correct_answer = correct_answers2[i][0]\n",
    "\n",
    "    column_value = row['question']\n",
    "\n",
    "    if answer not in correct_answer or str(correct_answer) not in answer:\n",
    "        incorrect_questions.append(column_value)\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = list(zip(questions2, answers2, correct_answers2))\n",
    "\n",
    "headers = ['Questions', 'Answers', 'Correct Answers']\n",
    "\n",
    "print(tabulate(table_data, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "for i in range(0,df3.shape[0]):\n",
    "    answer = answers2[i]\n",
    "    correct_answer = correct_answers2[i][0]\n",
    "    #print(answer)\n",
    "    #print(correct_answer)\n",
    "    if answer in correct_answer or str(correct_answer) in answer:\n",
    "        pos+=1\n",
    "    else:\n",
    "        neg+=1\n",
    "acc_rate = 100*(pos/(pos+neg))\n",
    "print(\"accuracy rate = \",acc_rate,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(0,df3.shape[0]):\n",
    "    answer = answers2[i]\n",
    "    correct_answer = correct_answers2[i]\n",
    "    if answer in correct_answer or str(correct_answer) in answer:\n",
    "        if (answer == correct_answer).any():\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "    else:\n",
    "        if answer != \"\":\n",
    "            FN += 1\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(TP)\n",
    "print(FP)\n",
    "print(FN)\n",
    "print(\"precision = \",precision)\n",
    "print(\"recall = \",recall)\n",
    "print(\"f1 score = \",f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for i in range(0, len(answers2)):\n",
    "    answer = answers2[i]\n",
    "    correct_answer = correct_answers2[i]\n",
    "    is_match = False\n",
    "    for inner_list in correct_answer:\n",
    "        for element in inner_list:\n",
    "            if answer in element or element in answer:\n",
    "                is_match = True\n",
    "                break\n",
    "        if is_match:\n",
    "            break\n",
    "    if is_match:\n",
    "        true_positives += 1\n",
    "    else:\n",
    "        false_negatives += 1\n",
    "\n",
    "    is_match = False\n",
    "    for inner_list in correct_answer:\n",
    "        for element in inner_list:\n",
    "            #print(element)\n",
    "            #print(str(correct_answer))\n",
    "            if element in correct_answer or str(correct_answer) in element:\n",
    "                is_match = True\n",
    "                print(element)\n",
    "                print(str(correct_answer))\n",
    "                break\n",
    "        if is_match:\n",
    "            break\n",
    "    if not is_match:\n",
    "        false_positives += 1\n",
    "true_negatives = 10 - true_positives - false_positives - false_negatives\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"true positive \",true_positives)\n",
    "print(\"false positive \",false_positives)\n",
    "print(\"false negative \",false_negatives)\n",
    "print(\"true negative \",true_negatives)\n",
    "\n",
    "print(\"precision = \", precision)\n",
    "print(\"recall = \", recall)\n",
    "print(\"F1 score = \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import squad_convert_examples_to_features\n",
    "from transformers import SquadV2Processor, squad_convert_examples_to_features\n",
    "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load SQuAD 2.0 dataset\n",
    "processor = SquadV2Processor()\n",
    "examples = load_dataset(\"squad_v2\",split=\"validation\")\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize and convert examples to features\n",
    "features, dataset = squad_convert_examples_to_features(\n",
    "    examples=examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=384,\n",
    "    doc_stride=128,\n",
    "    max_query_length=64,\n",
    "    is_training=False,\n",
    "    return_dataset=\"pt\",\n",
    "    threads=1,\n",
    ")\n",
    "\n",
    "# Create DataLoader for the dataset\n",
    "data_loader = DataLoader(dataset, batch_size=8)\n",
    "\n",
    "# Function to check if a question is unanswerable\n",
    "def is_unanswerable(scores, start_threshold=0.5, end_threshold=0.5):\n",
    "    start_prob, start_index = torch.max(scores[0], dim=1)\n",
    "    end_prob, end_index = torch.max(scores[1], dim=1)\n",
    "    return start_prob.item() < start_threshold and end_prob.item() < end_threshold\n",
    "\n",
    "# Extract unanswerable questions\n",
    "unanswerable_questions = []\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(data_loader, desc=\"Extracting Unanswerable Questions\"):\n",
    "        inputs = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = model(inputs, attention_mask=attention_mask)\n",
    "        start_scores, end_scores = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "        # Check if the question is unanswerable\n",
    "        for i in range(len(batch['example_id'])):\n",
    "            if is_unanswerable((start_scores[i], end_scores[i])):\n",
    "                unanswerable_questions.append(examples[batch['example_id'][i]].question_text)\n",
    "\n",
    "# Print some extracted unanswerable questions\n",
    "for i, question in enumerate(unanswerable_questions[:10]):\n",
    "    print(f\"{i+1}. {question}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
